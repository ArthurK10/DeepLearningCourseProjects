{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MMN12_Q2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "A_k8Fsk129ZT"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from random import randint"
      ],
      "metadata": {
        "id": "-4XhKjjHXLZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**sections A+B+C**"
      ],
      "metadata": {
        "id": "JYxESwDmg7K7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAVNKI1Jki9m"
      },
      "outputs": [],
      "source": [
        "class DropNorm(nn.Module):\n",
        "  def __init__(self, in_features):\n",
        "    super().__init__()\n",
        "    self.epsilon = 10**-5\n",
        "    self.beta = torch.rand(1)\n",
        "    self.gamma = torch.rand(1)\n",
        "\n",
        "  def forward(self, x):\n",
        "      #flattening the tensor from the first dim and onward\n",
        "      x = torch.flatten(x, start_dim=1)\n",
        "      #creating a numpy array in the size of  half of the flattend x filled with shuffled numbers\n",
        "      #will be used in the mask creation\n",
        "      l = np.arange(x.shape[1])\n",
        "      np.random.shuffle(l)\n",
        "      l = l[:int(x.shape[1]/2)]\n",
        "\n",
        "      #creating the mask. in every column l will get 1  \n",
        "      mask = torch.zeros_like(x)\n",
        "      mask[:,l] = 1\n",
        "      #multiplying x with mask to get x_hat\n",
        "      x_hat = mask*x\n",
        "\n",
        "      #while in traing mode, calculating mu, sigma, x_i and y_i\n",
        "      if self.training:\n",
        "        mu = torch.mean(x_hat, axis=0)\n",
        "        sigma = torch.std(x_hat, axis=0)\n",
        "        x_i = (x_hat-mu)/torch.sqrt(sigma + self.epsilon) \n",
        "        y_i = (x_i*self.gamma + self.beta)*mask\n",
        "      #eval mode\n",
        "      else:\n",
        "        mu = torch.zeros_like(x)\n",
        "        sigma = torch.ones_like(x)\n",
        "        x_i = (x_hat-mu)/torch.sqrt(sigma + self.epsilon) \n",
        "        y_i = (x_i*self.gamma + self.beta)*mask\n",
        "\n",
        "      return y_i"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.randn([2,3,4])\n",
        "print(t)"
      ],
      "metadata": {
        "id": "MKeZMVM8Edho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb828013-4385-4ba7-ba3c-eb2532051a0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1079,  3.4805,  1.1454, -0.2593],\n",
            "         [ 0.2618, -0.7093,  0.7214, -0.6537],\n",
            "         [ 0.4830, -0.8616,  1.7557,  0.1442]],\n",
            "\n",
            "        [[-1.7915, -0.6267,  0.1580, -0.9848],\n",
            "         [-0.8256,  1.2848, -0.0918,  1.5473],\n",
            "         [ 0.1307, -0.5738,  0.1145,  0.9436]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drop = DropNorm(4)\n",
        "drop.forward(t)"
      ],
      "metadata": {
        "id": "uIUMJjdeVbEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2537b6b1-26ca-4891-ecd5-0394879790a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  1.6223,  1.2069,  1.1499,  0.0000,  0.2395,  1.1700,  0.2108,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000, -0.0076,  0.4078,  0.4649,  0.0000,  1.3752,  0.4447,  1.4040,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drop.eval()\n",
        "drop.forward(t)"
      ],
      "metadata": {
        "id": "AqQpQ4Ty3HPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e09cc5c-007a-4ac9-aa11-c90dc36a7fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.8803,  3.1612,  0.0000,  0.0000,  0.0000,  0.0000,  1.2953,  0.0000,\n",
              "          1.1340,  0.2247,  1.9947,  0.0000],\n",
              "        [-0.4042,  0.3836,  0.0000,  0.0000,  0.0000,  0.0000,  0.7453,  0.0000,\n",
              "          0.8958,  0.4193,  0.8848,  0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**section D**"
      ],
      "metadata": {
        "id": "A_k8Fsk129ZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5pbVz7Aay2b",
        "outputId": "67a261b9-2dbe-4437-f6f9-3ad831823eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading the FashionMNIST dataset\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "#loading the data using dataloader\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)"
      ],
      "metadata": {
        "id": "F1DinVhFD2D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the class with my model. the model consists of linear aggregation fanctios, relu, batch normaliztion and dropout layers  \n",
        "class my_Fashion_MNIST(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(my_Fashion_MNIST, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "    #the forward pass\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = my_Fashion_MNIST()"
      ],
      "metadata": {
        "id": "twTUsEYZEFEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the training loop \n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        #Computing prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        #Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "hEu5XnA8EdY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the testing loop \n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error:\\nAccuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "7rQCUkG7EgPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 20\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h92Nb0UPEiTC",
        "outputId": "23e5e3f9-8496-450e-86da-235082ab5f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.303611  [    0/60000]\n",
            "loss: 2.121288  [ 6400/60000]\n",
            "loss: 1.918516  [12800/60000]\n",
            "loss: 1.781844  [19200/60000]\n",
            "loss: 1.557962  [25600/60000]\n",
            "loss: 1.448539  [32000/60000]\n",
            "loss: 1.361832  [38400/60000]\n",
            "loss: 1.278526  [44800/60000]\n",
            "loss: 1.289499  [51200/60000]\n",
            "loss: 1.181979  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 69.2%, Avg loss: 1.112020 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.124322  [    0/60000]\n",
            "loss: 1.104470  [ 6400/60000]\n",
            "loss: 0.897960  [12800/60000]\n",
            "loss: 1.116582  [19200/60000]\n",
            "loss: 0.882270  [25600/60000]\n",
            "loss: 0.907101  [32000/60000]\n",
            "loss: 0.913109  [38400/60000]\n",
            "loss: 0.892682  [44800/60000]\n",
            "loss: 0.929790  [51200/60000]\n",
            "loss: 0.847502  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 73.7%, Avg loss: 0.817646 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.777251  [    0/60000]\n",
            "loss: 0.817693  [ 6400/60000]\n",
            "loss: 0.607776  [12800/60000]\n",
            "loss: 0.918204  [19200/60000]\n",
            "loss: 0.745086  [25600/60000]\n",
            "loss: 0.715745  [32000/60000]\n",
            "loss: 0.772086  [38400/60000]\n",
            "loss: 0.752791  [44800/60000]\n",
            "loss: 0.771082  [51200/60000]\n",
            "loss: 0.762506  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 75.9%, Avg loss: 0.702987 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.612671  [    0/60000]\n",
            "loss: 0.696194  [ 6400/60000]\n",
            "loss: 0.494589  [12800/60000]\n",
            "loss: 0.795945  [19200/60000]\n",
            "loss: 0.706230  [25600/60000]\n",
            "loss: 0.667350  [32000/60000]\n",
            "loss: 0.706969  [38400/60000]\n",
            "loss: 0.686631  [44800/60000]\n",
            "loss: 0.704227  [51200/60000]\n",
            "loss: 0.657606  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 77.8%, Avg loss: 0.640463 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.559768  [    0/60000]\n",
            "loss: 0.638849  [ 6400/60000]\n",
            "loss: 0.428882  [12800/60000]\n",
            "loss: 0.690692  [19200/60000]\n",
            "loss: 0.693669  [25600/60000]\n",
            "loss: 0.570250  [32000/60000]\n",
            "loss: 0.617195  [38400/60000]\n",
            "loss: 0.703732  [44800/60000]\n",
            "loss: 0.666448  [51200/60000]\n",
            "loss: 0.631692  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 79.1%, Avg loss: 0.597884 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.544745  [    0/60000]\n",
            "loss: 0.573059  [ 6400/60000]\n",
            "loss: 0.389471  [12800/60000]\n",
            "loss: 0.663585  [19200/60000]\n",
            "loss: 0.645069  [25600/60000]\n",
            "loss: 0.578687  [32000/60000]\n",
            "loss: 0.599556  [38400/60000]\n",
            "loss: 0.640020  [44800/60000]\n",
            "loss: 0.629413  [51200/60000]\n",
            "loss: 0.616555  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 79.8%, Avg loss: 0.572222 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.478715  [    0/60000]\n",
            "loss: 0.556115  [ 6400/60000]\n",
            "loss: 0.391041  [12800/60000]\n",
            "loss: 0.648114  [19200/60000]\n",
            "loss: 0.576936  [25600/60000]\n",
            "loss: 0.542119  [32000/60000]\n",
            "loss: 0.535980  [38400/60000]\n",
            "loss: 0.642416  [44800/60000]\n",
            "loss: 0.620550  [51200/60000]\n",
            "loss: 0.572689  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 80.5%, Avg loss: 0.547751 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.450265  [    0/60000]\n",
            "loss: 0.492669  [ 6400/60000]\n",
            "loss: 0.357714  [12800/60000]\n",
            "loss: 0.641861  [19200/60000]\n",
            "loss: 0.526634  [25600/60000]\n",
            "loss: 0.527358  [32000/60000]\n",
            "loss: 0.494911  [38400/60000]\n",
            "loss: 0.606539  [44800/60000]\n",
            "loss: 0.675187  [51200/60000]\n",
            "loss: 0.525581  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 81.3%, Avg loss: 0.531745 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.402293  [    0/60000]\n",
            "loss: 0.501236  [ 6400/60000]\n",
            "loss: 0.345061  [12800/60000]\n",
            "loss: 0.564673  [19200/60000]\n",
            "loss: 0.523111  [25600/60000]\n",
            "loss: 0.479041  [32000/60000]\n",
            "loss: 0.516244  [38400/60000]\n",
            "loss: 0.646913  [44800/60000]\n",
            "loss: 0.617502  [51200/60000]\n",
            "loss: 0.559839  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 81.7%, Avg loss: 0.518490 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.449415  [    0/60000]\n",
            "loss: 0.492578  [ 6400/60000]\n",
            "loss: 0.334521  [12800/60000]\n",
            "loss: 0.555519  [19200/60000]\n",
            "loss: 0.509818  [25600/60000]\n",
            "loss: 0.514346  [32000/60000]\n",
            "loss: 0.487839  [38400/60000]\n",
            "loss: 0.620125  [44800/60000]\n",
            "loss: 0.571237  [51200/60000]\n",
            "loss: 0.500214  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 81.9%, Avg loss: 0.504859 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.452821  [    0/60000]\n",
            "loss: 0.456813  [ 6400/60000]\n",
            "loss: 0.297767  [12800/60000]\n",
            "loss: 0.523188  [19200/60000]\n",
            "loss: 0.512765  [25600/60000]\n",
            "loss: 0.463788  [32000/60000]\n",
            "loss: 0.435769  [38400/60000]\n",
            "loss: 0.565962  [44800/60000]\n",
            "loss: 0.637029  [51200/60000]\n",
            "loss: 0.507029  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 82.1%, Avg loss: 0.499399 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.405357  [    0/60000]\n",
            "loss: 0.497129  [ 6400/60000]\n",
            "loss: 0.300272  [12800/60000]\n",
            "loss: 0.570785  [19200/60000]\n",
            "loss: 0.500698  [25600/60000]\n",
            "loss: 0.475964  [32000/60000]\n",
            "loss: 0.462909  [38400/60000]\n",
            "loss: 0.566984  [44800/60000]\n",
            "loss: 0.546449  [51200/60000]\n",
            "loss: 0.483196  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 82.5%, Avg loss: 0.484946 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.380514  [    0/60000]\n",
            "loss: 0.467707  [ 6400/60000]\n",
            "loss: 0.325757  [12800/60000]\n",
            "loss: 0.503333  [19200/60000]\n",
            "loss: 0.499450  [25600/60000]\n",
            "loss: 0.484994  [32000/60000]\n",
            "loss: 0.388153  [38400/60000]\n",
            "loss: 0.582904  [44800/60000]\n",
            "loss: 0.526980  [51200/60000]\n",
            "loss: 0.468968  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 82.6%, Avg loss: 0.483389 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.348021  [    0/60000]\n",
            "loss: 0.455711  [ 6400/60000]\n",
            "loss: 0.265187  [12800/60000]\n",
            "loss: 0.495090  [19200/60000]\n",
            "loss: 0.437414  [25600/60000]\n",
            "loss: 0.431638  [32000/60000]\n",
            "loss: 0.443487  [38400/60000]\n",
            "loss: 0.581230  [44800/60000]\n",
            "loss: 0.553186  [51200/60000]\n",
            "loss: 0.391606  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 82.7%, Avg loss: 0.477537 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.372098  [    0/60000]\n",
            "loss: 0.506468  [ 6400/60000]\n",
            "loss: 0.302448  [12800/60000]\n",
            "loss: 0.481942  [19200/60000]\n",
            "loss: 0.464843  [25600/60000]\n",
            "loss: 0.450530  [32000/60000]\n",
            "loss: 0.459746  [38400/60000]\n",
            "loss: 0.602008  [44800/60000]\n",
            "loss: 0.505577  [51200/60000]\n",
            "loss: 0.442202  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 83.4%, Avg loss: 0.467504 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.369690  [    0/60000]\n",
            "loss: 0.435133  [ 6400/60000]\n",
            "loss: 0.274959  [12800/60000]\n",
            "loss: 0.465609  [19200/60000]\n",
            "loss: 0.456068  [25600/60000]\n",
            "loss: 0.444563  [32000/60000]\n",
            "loss: 0.418941  [38400/60000]\n",
            "loss: 0.615711  [44800/60000]\n",
            "loss: 0.508766  [51200/60000]\n",
            "loss: 0.421605  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 83.2%, Avg loss: 0.462600 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.370978  [    0/60000]\n",
            "loss: 0.463172  [ 6400/60000]\n",
            "loss: 0.268758  [12800/60000]\n",
            "loss: 0.483698  [19200/60000]\n",
            "loss: 0.449697  [25600/60000]\n",
            "loss: 0.450814  [32000/60000]\n",
            "loss: 0.382200  [38400/60000]\n",
            "loss: 0.580930  [44800/60000]\n",
            "loss: 0.482730  [51200/60000]\n",
            "loss: 0.421586  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 83.9%, Avg loss: 0.456574 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.314849  [    0/60000]\n",
            "loss: 0.388825  [ 6400/60000]\n",
            "loss: 0.258670  [12800/60000]\n",
            "loss: 0.463439  [19200/60000]\n",
            "loss: 0.443685  [25600/60000]\n",
            "loss: 0.466086  [32000/60000]\n",
            "loss: 0.384432  [38400/60000]\n",
            "loss: 0.501440  [44800/60000]\n",
            "loss: 0.517236  [51200/60000]\n",
            "loss: 0.429534  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 83.7%, Avg loss: 0.452283 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.360284  [    0/60000]\n",
            "loss: 0.393957  [ 6400/60000]\n",
            "loss: 0.250455  [12800/60000]\n",
            "loss: 0.433853  [19200/60000]\n",
            "loss: 0.401212  [25600/60000]\n",
            "loss: 0.463084  [32000/60000]\n",
            "loss: 0.400547  [38400/60000]\n",
            "loss: 0.540297  [44800/60000]\n",
            "loss: 0.496169  [51200/60000]\n",
            "loss: 0.425701  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 84.0%, Avg loss: 0.443144 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.316456  [    0/60000]\n",
            "loss: 0.386531  [ 6400/60000]\n",
            "loss: 0.241553  [12800/60000]\n",
            "loss: 0.429183  [19200/60000]\n",
            "loss: 0.466186  [25600/60000]\n",
            "loss: 0.437531  [32000/60000]\n",
            "loss: 0.344965  [38400/60000]\n",
            "loss: 0.471042  [44800/60000]\n",
            "loss: 0.472905  [51200/60000]\n",
            "loss: 0.477841  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 84.0%, Avg loss: 0.443357 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**section E**"
      ],
      "metadata": {
        "id": "-jlYBK77D-KS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the class with my model. the model consists of linear aggregation fanctios, relu, batch normaliztion and dropout layers and now with my\n",
        "#own DropNorm layer!\n",
        "class my_Fashion_MNIST_drop(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(my_Fashion_MNIST_drop, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            DropNorm(0),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "    #the forward pass\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = my_Fashion_MNIST_drop()"
      ],
      "metadata": {
        "id": "y2d3AdqAEM0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the training loop \n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        #Computing prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        #Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "HrqrLpEUROUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the testing loop \n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error:\\nAccuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "cFp9tJKtRTgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 20\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df4hF0SzEUWN",
        "outputId": "7c2b9b25-0a32-4e08-97ed-5590ccd98832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.437863  [    0/60000]\n",
            "loss: 2.314165  [ 6400/60000]\n",
            "loss: 2.147945  [12800/60000]\n",
            "loss: 2.093630  [19200/60000]\n",
            "loss: 1.993560  [25600/60000]\n",
            "loss: 1.972484  [32000/60000]\n",
            "loss: 1.864919  [38400/60000]\n",
            "loss: 1.851994  [44800/60000]\n",
            "loss: 1.771791  [51200/60000]\n",
            "loss: 1.773345  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 52.1%, Avg loss: 1.737872 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.732660  [    0/60000]\n",
            "loss: 1.711645  [ 6400/60000]\n",
            "loss: 1.584982  [12800/60000]\n",
            "loss: 1.713439  [19200/60000]\n",
            "loss: 1.583695  [25600/60000]\n",
            "loss: 1.560463  [32000/60000]\n",
            "loss: 1.478341  [38400/60000]\n",
            "loss: 1.449304  [44800/60000]\n",
            "loss: 1.485398  [51200/60000]\n",
            "loss: 1.433247  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 62.7%, Avg loss: 1.401742 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.391400  [    0/60000]\n",
            "loss: 1.460500  [ 6400/60000]\n",
            "loss: 1.294608  [12800/60000]\n",
            "loss: 1.464745  [19200/60000]\n",
            "loss: 1.317314  [25600/60000]\n",
            "loss: 1.262475  [32000/60000]\n",
            "loss: 1.348982  [38400/60000]\n",
            "loss: 1.165582  [44800/60000]\n",
            "loss: 1.282970  [51200/60000]\n",
            "loss: 1.143023  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 67.0%, Avg loss: 1.199385 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.165813  [    0/60000]\n",
            "loss: 1.219361  [ 6400/60000]\n",
            "loss: 1.044950  [12800/60000]\n",
            "loss: 1.189998  [19200/60000]\n",
            "loss: 1.202124  [25600/60000]\n",
            "loss: 1.118447  [32000/60000]\n",
            "loss: 1.159583  [38400/60000]\n",
            "loss: 1.037450  [44800/60000]\n",
            "loss: 1.181478  [51200/60000]\n",
            "loss: 1.155141  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 68.8%, Avg loss: 1.060448 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.030407  [    0/60000]\n",
            "loss: 1.182654  [ 6400/60000]\n",
            "loss: 0.875143  [12800/60000]\n",
            "loss: 1.150411  [19200/60000]\n",
            "loss: 1.134164  [25600/60000]\n",
            "loss: 0.981203  [32000/60000]\n",
            "loss: 1.013837  [38400/60000]\n",
            "loss: 0.996352  [44800/60000]\n",
            "loss: 1.008296  [51200/60000]\n",
            "loss: 1.047102  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 70.4%, Avg loss: 0.969209 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.007734  [    0/60000]\n",
            "loss: 0.940629  [ 6400/60000]\n",
            "loss: 0.819707  [12800/60000]\n",
            "loss: 0.992337  [19200/60000]\n",
            "loss: 0.879765  [25600/60000]\n",
            "loss: 0.938093  [32000/60000]\n",
            "loss: 0.924496  [38400/60000]\n",
            "loss: 0.997628  [44800/60000]\n",
            "loss: 0.925270  [51200/60000]\n",
            "loss: 1.057119  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 71.4%, Avg loss: 0.905223 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.846896  [    0/60000]\n",
            "loss: 0.899015  [ 6400/60000]\n",
            "loss: 0.724281  [12800/60000]\n",
            "loss: 0.932055  [19200/60000]\n",
            "loss: 0.903529  [25600/60000]\n",
            "loss: 0.950957  [32000/60000]\n",
            "loss: 0.867873  [38400/60000]\n",
            "loss: 0.826294  [44800/60000]\n",
            "loss: 0.850172  [51200/60000]\n",
            "loss: 0.813411  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 72.5%, Avg loss: 0.859453 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.793579  [    0/60000]\n",
            "loss: 0.866626  [ 6400/60000]\n",
            "loss: 0.660048  [12800/60000]\n",
            "loss: 0.887537  [19200/60000]\n",
            "loss: 0.915022  [25600/60000]\n",
            "loss: 0.794792  [32000/60000]\n",
            "loss: 0.830407  [38400/60000]\n",
            "loss: 0.874277  [44800/60000]\n",
            "loss: 0.918902  [51200/60000]\n",
            "loss: 0.848436  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 73.5%, Avg loss: 0.809472 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.756434  [    0/60000]\n",
            "loss: 0.874461  [ 6400/60000]\n",
            "loss: 0.725180  [12800/60000]\n",
            "loss: 0.811438  [19200/60000]\n",
            "loss: 0.815908  [25600/60000]\n",
            "loss: 0.794788  [32000/60000]\n",
            "loss: 0.768565  [38400/60000]\n",
            "loss: 0.729756  [44800/60000]\n",
            "loss: 0.871427  [51200/60000]\n",
            "loss: 0.785478  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 73.8%, Avg loss: 0.777031 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.787446  [    0/60000]\n",
            "loss: 0.780920  [ 6400/60000]\n",
            "loss: 0.576418  [12800/60000]\n",
            "loss: 0.901212  [19200/60000]\n",
            "loss: 0.881512  [25600/60000]\n",
            "loss: 0.757580  [32000/60000]\n",
            "loss: 0.758547  [38400/60000]\n",
            "loss: 0.742046  [44800/60000]\n",
            "loss: 0.738236  [51200/60000]\n",
            "loss: 0.680320  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 74.9%, Avg loss: 0.748867 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.663259  [    0/60000]\n",
            "loss: 0.888777  [ 6400/60000]\n",
            "loss: 0.511861  [12800/60000]\n",
            "loss: 0.884362  [19200/60000]\n",
            "loss: 0.725337  [25600/60000]\n",
            "loss: 0.672161  [32000/60000]\n",
            "loss: 0.709642  [38400/60000]\n",
            "loss: 0.738735  [44800/60000]\n",
            "loss: 0.725549  [51200/60000]\n",
            "loss: 0.756025  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 75.4%, Avg loss: 0.725215 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.726761  [    0/60000]\n",
            "loss: 0.764406  [ 6400/60000]\n",
            "loss: 0.563451  [12800/60000]\n",
            "loss: 0.804252  [19200/60000]\n",
            "loss: 0.746646  [25600/60000]\n",
            "loss: 0.754652  [32000/60000]\n",
            "loss: 0.696166  [38400/60000]\n",
            "loss: 0.708116  [44800/60000]\n",
            "loss: 0.748612  [51200/60000]\n",
            "loss: 0.692462  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 75.2%, Avg loss: 0.713845 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.602593  [    0/60000]\n",
            "loss: 0.727350  [ 6400/60000]\n",
            "loss: 0.508363  [12800/60000]\n",
            "loss: 0.757111  [19200/60000]\n",
            "loss: 0.743520  [25600/60000]\n",
            "loss: 0.668943  [32000/60000]\n",
            "loss: 0.727716  [38400/60000]\n",
            "loss: 0.693970  [44800/60000]\n",
            "loss: 0.801824  [51200/60000]\n",
            "loss: 0.650169  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 76.2%, Avg loss: 0.693276 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.569465  [    0/60000]\n",
            "loss: 0.669712  [ 6400/60000]\n",
            "loss: 0.486766  [12800/60000]\n",
            "loss: 0.718285  [19200/60000]\n",
            "loss: 0.739438  [25600/60000]\n",
            "loss: 0.657956  [32000/60000]\n",
            "loss: 0.627619  [38400/60000]\n",
            "loss: 0.697587  [44800/60000]\n",
            "loss: 0.696232  [51200/60000]\n",
            "loss: 0.650680  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 77.3%, Avg loss: 0.664583 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.561533  [    0/60000]\n",
            "loss: 0.696950  [ 6400/60000]\n",
            "loss: 0.434891  [12800/60000]\n",
            "loss: 0.681913  [19200/60000]\n",
            "loss: 0.699386  [25600/60000]\n",
            "loss: 0.605909  [32000/60000]\n",
            "loss: 0.606031  [38400/60000]\n",
            "loss: 0.817719  [44800/60000]\n",
            "loss: 0.765307  [51200/60000]\n",
            "loss: 0.718074  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 77.5%, Avg loss: 0.658578 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.573464  [    0/60000]\n",
            "loss: 0.610344  [ 6400/60000]\n",
            "loss: 0.479972  [12800/60000]\n",
            "loss: 0.695273  [19200/60000]\n",
            "loss: 0.669183  [25600/60000]\n",
            "loss: 0.562613  [32000/60000]\n",
            "loss: 0.634717  [38400/60000]\n",
            "loss: 0.712948  [44800/60000]\n",
            "loss: 0.682134  [51200/60000]\n",
            "loss: 0.639904  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 77.5%, Avg loss: 0.649189 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.596002  [    0/60000]\n",
            "loss: 0.644417  [ 6400/60000]\n",
            "loss: 0.451357  [12800/60000]\n",
            "loss: 0.741775  [19200/60000]\n",
            "loss: 0.631394  [25600/60000]\n",
            "loss: 0.572478  [32000/60000]\n",
            "loss: 0.593777  [38400/60000]\n",
            "loss: 0.712187  [44800/60000]\n",
            "loss: 0.660176  [51200/60000]\n",
            "loss: 0.794276  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 77.8%, Avg loss: 0.633556 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.512303  [    0/60000]\n",
            "loss: 0.666857  [ 6400/60000]\n",
            "loss: 0.395040  [12800/60000]\n",
            "loss: 0.638707  [19200/60000]\n",
            "loss: 0.642665  [25600/60000]\n",
            "loss: 0.559348  [32000/60000]\n",
            "loss: 0.616043  [38400/60000]\n",
            "loss: 0.666303  [44800/60000]\n",
            "loss: 0.615971  [51200/60000]\n",
            "loss: 0.630578  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 78.2%, Avg loss: 0.622249 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.551257  [    0/60000]\n",
            "loss: 0.601573  [ 6400/60000]\n",
            "loss: 0.471850  [12800/60000]\n",
            "loss: 0.592671  [19200/60000]\n",
            "loss: 0.788776  [25600/60000]\n",
            "loss: 0.557623  [32000/60000]\n",
            "loss: 0.535936  [38400/60000]\n",
            "loss: 0.658347  [44800/60000]\n",
            "loss: 0.701291  [51200/60000]\n",
            "loss: 0.836518  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 78.6%, Avg loss: 0.617362 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.546350  [    0/60000]\n",
            "loss: 0.648273  [ 6400/60000]\n",
            "loss: 0.386024  [12800/60000]\n",
            "loss: 0.663876  [19200/60000]\n",
            "loss: 0.677064  [25600/60000]\n",
            "loss: 0.701041  [32000/60000]\n",
            "loss: 0.666290  [38400/60000]\n",
            "loss: 0.689736  [44800/60000]\n",
            "loss: 0.641790  [51200/60000]\n",
            "loss: 0.565398  [57600/60000]\n",
            "Test Error:\n",
            "Accuracy: 79.0%, Avg loss: 0.603805 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    }
  ]
}